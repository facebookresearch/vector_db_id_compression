{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem setup and discussion\n",
    "**tl;dr; When indices are attributed sequentially, we can compress the ids down to less than `0.1` bits per id (roughly `0.1%` of its original size), while still guaranteeing random access to the set of ids given a cluster index.**\n",
    "\n",
    "There are two different scenarios for index attribution: sequential, where indices are assigned to vectors according to the order they are added to the database, and non-sequential or external, where the value of the index is set by some mechanism external to the FAISS index.\n",
    "\n",
    "**Here we focus on the sequential case**\n",
    "\n",
    "Summary and take-aways:\n",
    "- Assuming the number of clusters is $m = \\sqrt{n}$, where $n$ is the database size, we can store the sequential index using $\\frac{\\log(n)}{8\\cdot\\sqrt{n}}$ bytes per id, using a uniform code (which is very fast). This scheme guarantees random access: given a cluster index, we can decode the vector ids in that cluster, without having to decode the ids of any other cluster.\n",
    "\n",
    "In what follows we first model the joint distribution over all possible clusterings to understand the lower bound on the number of bits for this scenario.\n",
    "We do this by looking at the joint distribution over all possible values for the collection of clusters (which are themselves sets of indices).\n",
    "We then introduce the constraint of random access (i.e., given a cluster, we need to decode the indices of that cluster without having to decode the indices of all clusters).\n",
    "\n",
    "## Knowing the cluster sizes is enough to describe the entire index in the sequential case.\n",
    "\n",
    "The sequential scenario, in code, is: if the database is `db: NDArray[np.floating] = ds.get_database()`, then vector `db[i]: NDArray[np.floating]` is assigned index `i`.\n",
    "The assumption here is that indices have no meaning outside the index, and can be changed at will.\n",
    "This assumption allows us to impose the following constraints on $I_j$, the set of ids for the $j$-th cluster with centroid $c_j$,  by relabeling the indices after training the database.\n",
    "- **Contiguous**: $I_j = [s_j, s_j + k_j)$ are always contiguous integer intervals with a starting value $s_j$\n",
    "- **Monotone**: $x \\in I_j$ and $y \\in I_{j+1}$ implies $x < y$ for any $j$.\n",
    "\n",
    "To frame this as a source coding problem, we can work with the distribution $P_{I^m}$, where $m$ is the total number of clusters, and $I^m = (I_0, \\dots, I_{m-1})$.\n",
    "First, consider the conditional distribution $P_{I^m \\vert K^m}(\\cdot \\vert k^m)$ modeling when the cluster sizes $K^m = k^m$ are known.\n",
    "This distribution is a delta function: there is only one possible sequence $I^m$ obeying our constraints, if the cluster sizes are known, and it is such that \n",
    "- $I_0 = [0, k_1)$\n",
    "- $I_1 = [k_1, k_1 + k_2)$\n",
    "- $I_2 = [k_1 + k_2, k_1 + k_2 + k_3)$\n",
    "- $â€¦$\n",
    "- $I_{m-1} = [n - k_m, n)$\n",
    "where `n: int = len(db)` is the database size.\n",
    "The index set of cluster $j$ can be fully specified by giving the starting index, $s_j = \\sum_{i=1}^{j-1} k_i$, and the size $k_i$.\n",
    "Note that the first starting point is always fixed, $s_1 = 0$, as well as the last endpoint, $\\sum_{i=1}^m k_i = n$,\n",
    "\n",
    "$P_{I^m \\vert K^m}(\\cdot \\vert k^m)$ is a delta function, hence, in theory, we can compress the index to $0$ bits if the cluster sizes are known.\n",
    "In other words, the cost of encoding the entire index is that of encoding the size of the clusters.\n",
    "\n",
    "To encode the index, we can store the array `start_ids = [0, s_1, s_2, \\dots, s_{n-1}, n]` in memory, uncompressed.\n",
    "To decode the ids of cluster `j`, we simply take `np.arange(start_ids[j], start_ids[j+1])`.\n",
    "\n",
    "The `start_ids` array will occupy roughly $\\frac{m\\cdot u}{8\\cdot n}$ bytes per id in memory, where $u$ is the size of the representation of the integer value in `start_ids`, and must obey $u \\geq \\log(n)$.\n",
    "Assuming $u = \\log(n)$ is possible, and $m = \\sqrt{n}$, then the cost per id is $\\frac{\\log(n)}{8 \\cdot \\sqrt{n}}$, which converges to $0$ as $n$ grows.\n",
    "For `n=1e6`, the cost is already very small, at $0.0025$ bytes per id, or $2.5$ kB for the entire index.\n",
    "\n",
    "It is possible to compress `start_ids` further, by entropy modelling the start values, but might not be worth it given the compressed database already hovers around $0.1\\%$ of its original size (see experiments below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from faiss.contrib.datasets import (\n",
    "    SyntheticDataset,\n",
    "    DatasetSIFT1M,\n",
    "    DatasetGIST1M\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from ipynb_helper_functions import (\n",
    "    prepare_index,\n",
    "    get_ivfs,\n",
    "    make_start_ids_array\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build index and relabel sequentially (small example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully relabelled index sequentially.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0, 1, 2]), array([3, 4, 5, 6]), array([7, 8, 9])]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([ 0,  3,  7, 10], dtype=uint32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[array([0, 1, 2]), array([3, 4, 5, 6]), array([7, 8, 9])]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initially, we take a small index to be able to view the relabelling of ids.\n",
    "ds = SyntheticDataset(d=2, nt=int(1e6), nb=10, nq=1)\n",
    "num_clusters = 3\n",
    "index_str = f\"IVF{num_clusters},SQ8\"\n",
    "\n",
    "# This function takes care of re-ordering the ids according to our constraints.\n",
    "index = prepare_index(ds, index_str, relabel_ids_sequentially=True)\n",
    "\n",
    "# Returns `list[NDArray[np.integer]]` containing the ids of cluster `j` at index `j`.\n",
    "ivfs = get_ivfs(index)\n",
    "\n",
    "# Process ivfs to get starting ids for each interval.\n",
    "start_ids = make_start_ids_array(index)\n",
    "\n",
    "# Reconstruct the ivfs from start_ids, which is what will be done at search time. \n",
    "ivfs_reconstructed = [\n",
    "    np.arange(start_ids[j], start_ids[j+1])\n",
    "    for j in range(len(start_ids) - 1)\n",
    "]\n",
    "\n",
    "display(ivfs, start_ids, ivfs_reconstructed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute compression ratio on larger indices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully relabelled index sequentially.\n",
      "Results for DatasetGIST1M w/ IVF1000,SQ8\n",
      "Original size: 64.00 bits per id\n",
      "Compressed size: 0.03 bits per id\n",
      "Compression ratio: 0.05% of original size\n",
      "----------\n",
      "Successfully relabelled index sequentially.\n",
      "Results for DatasetGIST1M w/ IVF2000,SQ8\n",
      "Original size: 64.00 bits per id\n",
      "Compressed size: 0.06 bits per id\n",
      "Compression ratio: 0.10% of original size\n",
      "----------\n",
      "Successfully relabelled index sequentially.\n",
      "Results for DatasetSIFT1M w/ IVF1000,SQ8\n",
      "Original size: 64.00 bits per id\n",
      "Compressed size: 0.03 bits per id\n",
      "Compression ratio: 0.05% of original size\n",
      "----------\n",
      "Successfully relabelled index sequentially.\n",
      "Results for DatasetSIFT1M w/ IVF2000,SQ8\n",
      "Original size: 64.00 bits per id\n",
      "Compressed size: 0.06 bits per id\n",
      "Compression ratio: 0.10% of original size\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for ds in [DatasetGIST1M(), DatasetSIFT1M()]:\n",
    "    for num_clusters in [1_000, 2_000]:\n",
    "        index_str = f\"IVF{num_clusters},SQ8\"\n",
    "        index = prepare_index(ds, index_str, relabel_ids_sequentially=True)\n",
    "        ivfs = get_ivfs(index)\n",
    "        start_ids = make_start_ids_array(index)\n",
    "        ivfs_reconstructed = [\n",
    "            np.arange(start_ids[j], start_ids[j+1])\n",
    "            for j in range(len(start_ids) - 1)\n",
    "        ]\n",
    "\n",
    "        # Assert reconstruction is correct\n",
    "        assert all(np.all(ivfs[j] == ivfs_reconstructed[j]) for j in range(num_clusters))\n",
    "\n",
    "        ivf_bytes = sum(ivf.nbytes for ivf in ivfs)\n",
    "        start_ids_bytes = start_ids.nbytes\n",
    "        n = index.ntotal\n",
    "        print(f'Results for {ds.__class__.__name__} w/ {index_str}')\n",
    "        print(f'Original size: {8*ivf_bytes/n:.2f} bits per id')\n",
    "        print(f'Compressed size: {8*start_ids_bytes/n:.2f} bits per id')\n",
    "        print(f'Compression ratio: {100*start_ids_bytes/ivf_bytes:.2f}% of original size')\n",
    "        print(10*'-')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_ivf_compression",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
