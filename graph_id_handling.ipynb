{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "951da0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "from faiss.contrib.datasets import SyntheticDataset, DatasetSIFT1M\n",
    "from faiss.contrib.inspect_tools import get_NSG_neighbors\n",
    "import bz2, io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "78bd1174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset in dimension 128, with metric L2, size: Q 10000 B 1000000 T 100000\n"
     ]
    }
   ],
   "source": [
    "# ds = SyntheticDataset(128, 0, 1_000_000, 10_000)\n",
    "ds = DatasetSIFT1M()\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb62e312",
   "metadata": {},
   "source": [
    "# Make an NSG index\n",
    "\n",
    "NSG is a graph based indexing method. It is less popular than HSNW but it is simpler because it's a single graph rather than a layered set of graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "63f3fa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the C++ index: a NSG graph with 16 neighbors per node\n",
    "index = faiss.index_factory(ds.d, \"NSG64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6617bdb8-4e0c-4b82-be3c-cea53b738be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.add(ds.get_database())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "17baf0bd-b82e-47f4-94de-f06e0ff93a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = get_NSG_neighbors(index.nsg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0139c5fa-31b9-4990-82e4-c9f685e28245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 64)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae991ef",
   "metadata": {},
   "source": [
    "# Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b67ebc45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entropy: 19.764543051025118\n",
      "log2(#ids): 19.931568569324174\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "import heapq\n",
    "\n",
    "type NDfloat = NDArray[np.floating]\n",
    "type NDint = NDArray[np.integer]\n",
    "\n",
    "def compute_distance[T: np.floating](x: NDArray[T], y: NDArray[T]) -> NDArray[T]:\n",
    "    '''Last axis must be vector dimensions.'''\n",
    "    return np.square(x - y).sum(axis=-1)\n",
    "\n",
    "def graph_search(graph: NDint, entrypoint: np.integer, query: np.floating, max_steps: int) -> tuple[NDint, NDfloat]:\n",
    "    visited_ids: set[np.integer] = set()\n",
    "    query = query[None, :] # shape=(edges, d)\n",
    "    path_ids: NDint = -1*np.ones(max_steps, dtype=np.int32)\n",
    "    path_distances: NDfloat = np.full(max_steps, np.nan, dtype=np.float32)\n",
    "    min_distance: float = float('inf')\n",
    "\n",
    "    for i in range(max_steps):\n",
    "        endpoint_ids = graph[entrypoint] # shape=(edges, d)\n",
    "        visited_ids.add(entrypoint)\n",
    "\n",
    "        # drop invalid ids\n",
    "        endpoint_ids = endpoint_ids[endpoint_ids != -1]\n",
    "        endpoint_values = index.reconstruct_batch(endpoint_ids) # shape=(edges, d)\n",
    "        distances = compute_distance(endpoint_values, query)  # shape=(edges,)\n",
    "\n",
    "        for j, rank_id in enumerate(np.argsort(distances)):\n",
    "            if endpoint_ids[rank_id] not in visited_ids and distances[rank_id] < min_distance:\n",
    "                path_ids[i] = endpoint_ids[rank_id]\n",
    "                path_distances[i] = min_distance = distances[rank_id]\n",
    "                entrypoint = endpoint_ids[rank_id]\n",
    "                break\n",
    "            elif distances[rank_id] >= min_distance:\n",
    "                break\n",
    "\n",
    "        if j == len(distances) - 1:\n",
    "            print(visited_ids)\n",
    "            print(path_ids)\n",
    "            print(entrypoint)\n",
    "            raise Exception\n",
    "\n",
    "    return path_ids, path_distances\n",
    "\n",
    "\n",
    "def greedy_routing(graph: NDint, entrypoint: np.integer, query: np.floating, max_steps: int, heap_size: int) -> tuple[NDint, NDfloat]:\n",
    "    heap: list[tuple[(np.float32, np.int32)]] = list()\n",
    "    visited_ids: set[np.integer] = set()\n",
    "    query = query[None, :] # shape=(edges, d)\n",
    "    path_ids: NDint = -1*np.ones(max_steps, dtype=np.int32)\n",
    "    path_distances: NDfloat = np.full(max_steps, np.nan, dtype=np.float32)\n",
    "\n",
    "    heapq.heappush(heap, (compute_distance(entrypoint, query), entrypoint))\n",
    "\n",
    "    for i in range(max_steps):\n",
    "        dist, v_id = heapq.heappop(heap)\n",
    "        endpoint_ids = graph[v_id] # shape=(edges, d)\n",
    "        visited_ids.add(v_id)\n",
    "\n",
    "        # drop invalid or visited ids\n",
    "        endpoint_ids = endpoint_ids[endpoint_ids != -1]\n",
    "        endpoint_ids = np.array([id for id in endpoint_ids if id not in visited_ids])\n",
    "\n",
    "        # compute distances\n",
    "        endpoint_values = index.reconstruct_batch(endpoint_ids) # shape=(edges, d)\n",
    "        distances = compute_distance(endpoint_values, query)  # shape=(edges,)\n",
    "\n",
    "        for j in range(len(distances)):\n",
    "            heapq.heappush(heap, (distances[j], endpoint_ids[j]))\n",
    "\n",
    "        path_ids[i] = endpoint_ids[j]\n",
    "\n",
    "    return path_ids, path_distances\n",
    "\n",
    "queries = ds.get_queries() # shape=(batch, d)\n",
    "max_steps = 100\n",
    "path_ids = -1*np.ones((queries.shape[0], max_steps), dtype=np.int32)\n",
    "for i, q in enumerate(queries):\n",
    "    path_ids[i], path_distances = graph_search(graph, np.int32(23), q, max_steps)\n",
    "    # plt.plot(path_distances)\n",
    "# plt.show()\n",
    "\n",
    "p_quantized = np.ones(index.ntotal)\n",
    "path_ids = path_ids.flatten()\n",
    "path_ids = path_ids[path_ids != -1]\n",
    "ids, freqs = np.unique(path_ids, return_counts=True)\n",
    "p_quantized[ids] = freqs\n",
    "p = p_quantized/p_quantized.sum()\n",
    "print('entropy:', -np.sum(p*np.log2(p)))\n",
    "print('log2(#ids):', np.log2(index.ntotal))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172ab0ae-a736-476c-91da-0da7c0a193e9",
   "metadata": {},
   "source": [
    "50k nodes, 16 outgoing edges per node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "005f568f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 2)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = ds.get_queries()\n",
    "index.reconstruct_batch(graph[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "53d0c49d-04e6-4fdb-8c9a-8277165094f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([36029, 49886, 34188, 28030, 45562, 15327,  8257, 21581,  8152,\n",
       "       13419, 10381, 49588, 34097, 27447,  6180, 13718], dtype=int32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph[123]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6123f64-7607-4fdf-9dc6-f5aaf07e8cbd",
   "metadata": {},
   "source": [
    "edges of node #123 -- they are ordered arbitrarily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1fe08686-e7cf-4473-9d46-88975cb5ad62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09029"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of -1s \n",
    "(graph == -1).sum() / graph.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0de45a-3b09-4545-8cfd-ef9f051e4522",
   "metadata": {},
   "source": [
    "9% invalid edges (represented as -1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "61d17f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 16)\n"
     ]
    }
   ],
   "source": [
    "queries = ds.get_queries()\n",
    "print(queries.shape)\n",
    "# reference results\n",
    "Dref, Iref = index.search(queries, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b977af51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6708, 16841, 13264, 42596, 21075],\n",
       "       [21695, 15810, 33083, 12244, 48081],\n",
       "       [ 1126, 26665, 17933, 49309, 39121],\n",
       "       [12169, 27365, 16199, 21937,  3238]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Iref  # show ids of the 5 nearest neighbors of each query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b91d16-5b15-4368-8c25-2c168ff0854e",
   "metadata": {},
   "source": [
    "# Order invariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7c25ec81-187e-4aaa-a126-2efc532cfb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort all edges in graph (ignoring -1s)\n",
    "for row in graph: \n",
    "    npos = (row >= 0).sum()\n",
    "    row[:npos].sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7e4b554a-960b-45f5-a1b8-e74aac0ae9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_NSG_neighbors(nsg, neighbors): \n",
    "    graph = nsg.get_final_graph()\n",
    "    assert neighbors.shape == (graph.N, graph.K)\n",
    "    assert neighbors.dtype == np.int32\n",
    "    faiss.memcpy(\n",
    "        graph.data,\n",
    "        faiss.swig_ptr(neighbors),\n",
    "        neighbors.nbytes\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d0cc5674-31bd-4aaa-9084-bf168524a866",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_NSG_neighbors(index.nsg, graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c576efa5-bdff-4207-aab9-fc28481c91b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6708, 16841, 13264, 42596, 21075],\n",
       "       [21695, 15810, 33083, 12244, 48081],\n",
       "       [ 1126, 26665, 17933, 49309, 39121],\n",
       "       [12169, 27365, 16199, 21937,  3238]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = ds.get_queries()\n",
    "Dnew, Inew = index.search(queries, 5)\n",
    "Inew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8f5a6046-52e6-4fa2-90ce-0c032fd97f52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(Inew == Iref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b3ed1e-4e70-47be-90e8-10fe77ef8dd1",
   "metadata": {},
   "source": [
    "# Naive compression of the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1fb9cc62-0d2f-4312-9f45-66d8e1f5c62d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3200000"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# raw size in memory\n",
    "graph.nbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "901be9a6-13df-4ef2-8478-d4e54acdacda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1600000.0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if we use only the necessary number of bits \n",
    "bits_per_vector = np.ceil(np.log2(index.ntotal + 1))\n",
    "graph.size * bits_per_vector / 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b918f4c7-47a7-4a17-af11-6e4762d7bb9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1505536.0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if we don't store the -1s (but we need to store the nb valid entries per vector)\n",
    "bits_per_vector = np.ceil(np.log2(index.ntotal))\n",
    "(graph != -1).sum() * bits_per_vector / 8 + graph.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8cb9fa1c-e063-4ed3-b621-e86278e72921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1336870"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compress with generic compressor \n",
    "buf = io.BytesIO()\n",
    "with bz2.open(buf, mode=\"wb\") as f: \n",
    "    f.write(graph.tobytes())\n",
    "len(buf.getbuffer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0289e895-cf0b-45e0-bec4-4335f5747623",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a09fe36",
   "metadata": {},
   "source": [
    "# Do indices correlate with a set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "ffe466d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset in dimension 128, with metric L2, size: Q 10000 B 1000000 T 100000\n"
     ]
    }
   ],
   "source": [
    "# ds = SyntheticDataset(128, 0, 1_000_000, 10_000)\n",
    "ds = DatasetSIFT1M()\n",
    "print(ds)\n",
    "\n",
    "# create the C++ index: a NSG graph with 16 neighbors per node\n",
    "index = faiss.index_factory(ds.d, \"NSG64\")\n",
    "index.add(ds.get_database())\n",
    "graph = get_NSG_neighbors(index.nsg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "945fa272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "from faiss.contrib import datasets\n",
    "from faiss.contrib.inspect_tools import get_NSG_neighbors\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def average_gap_value(arr: NDArray[np.integer]) -> NDArray[np.integer]:\n",
    "    '''Any negative value is considered invalid. `arr` should have shape (batch, sequence).'''\n",
    "\n",
    "    has_at_least_2_valid_entries = (arr >= 0).sum(axis=1) >= 2\n",
    "    arr = arr[has_at_least_2_valid_entries, :]\n",
    "    gaps = np.diff(arr, axis=1)\n",
    "    gap_sum = np.where(arr[:, :-1] >= 0, gaps, 0).sum(axis=1)\n",
    "    valid_entries = (arr >= 0).sum(axis=1)\n",
    "    return gap_sum/(valid_entries - 1)\n",
    "\n",
    "def test_average_gap_value():\n",
    "    arr = np.arange(100).reshape(10, 10)\n",
    "    assert average_gap_value(arr).mean() == 1.0\n",
    "    assert average_gap_value(23*arr).mean() == 23\n",
    "\n",
    "    arr = np.arange(10).reshape(2, 5)\n",
    "    arr[0, :2] = -1 # -1, -1, 2, 3, 4 -> 1, 1 -> 1\n",
    "    arr[1, :3] = -1 # -1, -1, -1, 8, 9 -> 1, 1 -> 1\n",
    "    assert average_gap_value(arr).mean() == 1.0\n",
    "\n",
    "    # -1, -1, 6, 9, 12 -> 3, 3 -> 3\n",
    "    # -1, -1, -1, 24, 27 -> 3 -> 3\n",
    "    assert average_gap_value(3*arr).mean() == 3.0\n",
    "\n",
    "def test_average_gap_value_uniform():\n",
    "    np.random.seed(0)\n",
    "    max_value = (1 << 32) - 1\n",
    "    arr = np.sort(np.random.randint(0, max_value + 1, size=(5*1000)).reshape((5, 1000)), axis=1)\n",
    "\n",
    "    avg_num_gaps = ((arr >= 0).sum(axis=1) - 1).mean()\n",
    "    assert np.all(avg_num_gaps == 999)\n",
    "\n",
    "    uniform_gap_value_from_bound = max_value/avg_num_gaps\n",
    "    uniform_gap_value_from_max = arr.max()/avg_num_gaps \n",
    "    assert 0.98 < uniform_gap_value_from_bound/uniform_gap_value_from_max < 1.02\n",
    "\n",
    "    avg_gap_value = average_gap_value(arr).mean()\n",
    "    assert 0.98 < avg_gap_value/uniform_gap_value_from_max < 1.02\n",
    "\n",
    "test_average_gap_value()\n",
    "test_average_gap_value_uniform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141717ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gap_statistics(ds, index_str='NSG64'):\n",
    "    index = faiss.index_factory(ds.d, index_str)\n",
    "    index.add(ds.get_database())\n",
    "\n",
    "    graph = np.sort(get_NSG_neighbors(index.nsg), axis=1)\n",
    "    avg_num_gaps = ((graph >= 0).sum(axis=1) - 1).mean()\n",
    "\n",
    "    avg_gap_value = average_gap_value(graph).mean()\n",
    "    uniform_gap_value = graph.max()/avg_num_gaps\n",
    "\n",
    "    print('index_str:', index_str, 'Dataset:', ds.__class__.__name__, 'Average gap value of SIFT1M:', avg_gap_value, 'max/avg_of_gaps:', uniform_gap_value, 'gaps ratio': avg_gap_value/uniform_gap_value)\n",
    "\n",
    "    return avg_gap_value, uniform_gap_value\n",
    "\n",
    "datasets_space = [\n",
    "    datasets.SyntheticDataset(128, 0, 1_000_000, 10_000),\n",
    "    datasets.DatasetSIFT1M(),\n",
    "    datasets.DatasetGIST1M()\n",
    "]\n",
    "\n",
    "index_str_space = [\n",
    "    'NSG64', 'NSG32', 'NSG16', 'NSG8'\n",
    "]\n",
    "\n",
    "# results = Parallel(n_jobs=os.cpu_count())([\n",
    "#     delayed(compute_gap_statistics)(ds, index_str)\n",
    "#     for ds in datasets_space\n",
    "#     for index_str in index_str_space\n",
    "# ])\n",
    "\n",
    "results = [\n",
    "    compute_gap_statistics(ds, index_str)\n",
    "    for ds in datasets_space\n",
    "    for index_str in index_str_space\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
